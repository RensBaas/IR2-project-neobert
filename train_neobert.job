#!/bin/bash

#SBATCH --partition=gpu_a100
#SBATCH --gpus=1
#SBATCH --job-name=neobert_msmarco
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=18
#SBATCH --time=34:00:00
#SBATCH --output=logs/train_neobert_%j.out
#SBATCH --error=logs/train_neobert_%j.err

module purge
module load 2024
module load Anaconda3/2024.06-1

source $HOME/tevatron_env/bin/activate

# Set cache directories
export HF_HOME=$HOME/.cache/huggingface
export TRANSFORMERS_CACHE=$HOME/.cache/huggingface/transformers
export HF_DATASETS_CACHE=$HOME/.cache/huggingface/datasets

export TRUST_REMOTE_CODE=True

# Create output directory 
mkdir -p $HOME/model_msmarco_neobert
mkdir -p logs

# Print some info
echo "Job started at: $(date)"
echo "Running on node: $(hostname)"
echo "GPU info:"
nvidia-smi

# Run training
python $HOME/train_neobert_custom.py \
  --output_dir $HOME/model_msmarco_neobert \
  --model_name_or_path chandar-lab/NeoBERT \
  --save_steps 10000 \
  --dataset_name Tevatron/msmarco-passage \
  --fp16 \
  --per_device_train_batch_size 32 \
  --train_n_passages 8 \
  --dataloader_num_workers 8 \
  --learning_rate 1e-5 \
  --q_max_len 32 \
  --p_max_len 128 \
  --num_train_epochs 3 \
  --logging_steps 500 \
  --overwrite_output_dir

echo "Job finished at: $(date)"